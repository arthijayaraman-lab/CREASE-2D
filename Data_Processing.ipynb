{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3b8b5f6-7895-4c25-b2db-6898ec5d1b84",
      "metadata": {
        "id": "b3b8b5f6-7895-4c25-b2db-6898ec5d1b84"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import shutil\n",
        "import re\n",
        "import csv\n",
        "from itertools import product\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZM89R7Yqisn",
        "outputId": "ec539e37-b8e7-4a5d-c5cd-7c39d047c3a6"
      },
      "id": "qZM89R7Yqisn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split the scattering profiles into train and test\n",
        "random.seed(51)\n",
        "#xy folder contains 3000 scattering profile files\n",
        "input_folder = \"/content/drive/MyDrive/CREASE-2D/xy\"\n",
        "#output paths to separate train and test\n",
        "output_train_folder = \"/content/drive/MyDrive/CREASE-2D/Crease_2400_126/train\"\n",
        "output_test_folder = \"/content/drive/MyDrive/CREASE-2D/Crease_2400_126/test\"\n",
        "all_files = os.listdir(input_folder)\n",
        "random.shuffle(all_files)\n",
        "total_files = len(all_files)\n",
        "train_size = 2400\n",
        "test_size = total_files - train_size\n",
        "train_files = all_files[:train_size]\n",
        "test_files = all_files[train_size:]\n",
        "sample_id_info = []\n",
        "def copy_files(files, source_folder, destination_folder):\n",
        "    for file_name in files:\n",
        "        source_path = Path(source_folder) / file_name\n",
        "        destination_path = Path(destination_folder) / file_name\n",
        "        #Here original scattering profiles are copied to create train and test folders. We can also move them instead of copying to make it faster\n",
        "        shutil.copy2(source_path, destination_path)\n",
        "#we idientifed each scattering profile with unique sample id's given as file names\n",
        "def collect_sample_ids(files, destination_folder):\n",
        "    '''\n",
        "    function extracts sample ID from file name and assigned folder either train or test\n",
        "    '''\n",
        "    for file_name in files:\n",
        "        sample_id = file_name.split('_')[1]\n",
        "        sample_id_info.append({\n",
        "            'Sample ID': sample_id,\n",
        "            'Folder': destination_folder\n",
        "        })\n",
        "#creates output folders\n",
        "os.makedirs(output_train_folder, exist_ok=True)\n",
        "os.makedirs(output_test_folder, exist_ok=True)\n",
        "#copy files to output folders\n",
        "copy_files(train_files, input_folder, output_train_folder)\n",
        "copy_files(test_files, input_folder, output_test_folder)\n",
        "#collect sample IDs and specific folder names\n",
        "collect_sample_ids(train_files, output_train_folder)\n",
        "collect_sample_ids(test_files, output_test_folder)\n",
        "csv_file_path = \"/content/drive/MyDrive/CREASE-2D/Crease_2400_126/sample_id_info.csv\"\n",
        "sample_id_df = pd.DataFrame(sample_id_info)\n",
        "sample_id_df.to_csv(csv_file_path, index=False)"
      ],
      "metadata": {
        "id": "uJOV8jw056JZ"
      },
      "id": "uJOV8jw056JZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9c408fd-8db3-44b0-850a-57fbef3fa8cc",
      "metadata": {
        "id": "a9c408fd-8db3-44b0-850a-57fbef3fa8cc"
      },
      "outputs": [],
      "source": [
        "#This contains 6 input features used to generate 3D structure\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/CREASE-2D/Features_6.csv')\n",
        "#This csv file contains all the sample IDs and their locations\n",
        "df2 = pd.read_csv('/content/drive/MyDrive/CREASE-2D/Crease_2400_126/sample_id_info.csv')\n",
        "unique_folders = df2['Folder'].unique()\n",
        "dfs_by_folder = {}\n",
        "#identify the train and test features and separate them\n",
        "for folder in unique_folders:\n",
        "    sample_ids = df2.loc[df2['Folder'] == folder, 'Sample ID']\n",
        "    df_temp = df1[df1['Sample ID'].isin(sample_ids)]\n",
        "    dfs_by_folder[folder] = df_temp\n",
        "df_train = dfs_by_folder['/content/drive/MyDrive/CREASE-2D/Crease_2400_126/train']\n",
        "df_test = dfs_by_folder['/content/drive/MyDrive/CREASE-2D/Crease_2400_126/test']\n",
        "df_train.to_csv('/content/drive/MyDrive/CREASE-2D/Crease_2400_126/train/train_features.csv',index=False)\n",
        "df_test.to_csv('/content/drive/MyDrive/CREASE-2D/Crease_2400_126/test/test_features.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f763f95-76ae-473a-884d-35e055033f5d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f763f95-76ae-473a-884d-35e055033f5d",
        "outputId": "7567dc9e-e2c2-4177-e26d-0a7ff9caccf0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11188800, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#create a grid to locate each intensity value\n",
        "theta = np.linspace(0, np.pi, 37)\n",
        "log_q = np.linspace(-2,3,126)\n",
        "combinations = list(product(log_q,theta))\n",
        "log_q_mesh, theta_mesh = np.meshgrid(log_q,theta,indexing='ij')\n",
        "#add these grid to each set of features\n",
        "combined_data = df_train.loc[np.repeat(df_train.index.values,len(combinations))]\n",
        "combined_data['log_q'] = np.tile(log_q_mesh.ravel(),len(df_train))\n",
        "combined_data['theta'] = np.tile(theta_mesh.ravel(),len(df_train))\n",
        "combined_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94a0c8f9-8b0c-4785-a4bf-b7a54bbda58d",
      "metadata": {
        "id": "94a0c8f9-8b0c-4785-a4bf-b7a54bbda58d"
      },
      "outputs": [],
      "source": [
        "#grid based sub-sampling on scattering profiles\n",
        "input_ = '/content/drive/MyDrive/CREASE-2D/Crease_2400_126/train'\n",
        "output_ = '/content/drive/MyDrive/CREASE-2D/Crease_2400_126/downsampled_train_126/'\n",
        "if not os.path.exists(output_):\n",
        "    os.makedirs(output_)\n",
        "for f in os.listdir(input_):\n",
        "    if f.endswith('xy.txt'):\n",
        "        inp_path = os.path.join(input_, f)\n",
        "        print(f\"Processing: {inp_path}\")\n",
        "        out_path = os.path.join(output_, f)\n",
        "        image_data = np.loadtxt(inp_path, delimiter=',')\n",
        "        downsampled = np.zeros((126, 37))\n",
        "        for i in range(126):\n",
        "            for j in range(37):\n",
        "                #subsample a row from every 4th row and a column from every 5th column\n",
        "                downsampled[i, j] = image_data[i * 4, j * 5]\n",
        "        np.savetxt(out_path, downsampled, delimiter=',')\n",
        "        print(f\"Saved: {out_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6436bfec-442b-4701-8d08-74f9a004fff5",
      "metadata": {
        "id": "6436bfec-442b-4701-8d08-74f9a004fff5",
        "outputId": "d1c7f81f-6f00-41b9-8fa7-9ec48faf2705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(11188800, 10)\n"
          ]
        }
      ],
      "source": [
        "#extract intensities with (q,theta) from subsampled files\n",
        "data_folder = \"/content/drive/MyDrive/CREASE-2D/Crease_2400_126/downsampled_train_126/\"\n",
        "data = []\n",
        "for filename in os.listdir(data_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        match = re.search(r'\\d+', filename)\n",
        "        #add sample ID as well to indentify as unique id\n",
        "        first_number = int(match.group()) if match else None\n",
        "        file_path = os.path.join(data_folder, filename)\n",
        "        matrix_data = []\n",
        "        with open(file_path, \"r\") as txt_file:\n",
        "            for line in txt_file:\n",
        "                values = line.strip().split(\",\")\n",
        "                row = list(map(float, values))\n",
        "                matrix_data.append(row)\n",
        "        #flatten the matrix and and add locations of it using q and theta\n",
        "        flattened_data = [value for row in matrix_data for value in row]\n",
        "        combinations = [(log_q_value, theta_value) for log_q_value in log_q for theta_value in theta]\n",
        "        for idx, (log_q_value, theta_value) in enumerate(combinations):\n",
        "            intensity = flattened_data[idx]\n",
        "            data.append([log_q_value, theta_value, intensity, first_number])\n",
        "#create a dataframe of location q and theta with intensity values\n",
        "df1 = pd.DataFrame(data, columns=[\"log_q\", \"theta\", \"I_q\", \"Sample ID\"])\n",
        "#combine the features, q, theta and intensty to form train dataset\n",
        "train_df = pd.merge(combined_data, df1, on=['Sample ID','log_q','theta'], sort=True, copy=False)\n",
        "print(train_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3680c52-9d72-4568-92d9-0b41174fffa7",
      "metadata": {
        "id": "d3680c52-9d72-4568-92d9-0b41174fffa7"
      },
      "outputs": [],
      "source": [
        "#All the above process that's been doing on train set is done for test set here\n",
        "test_data = df_test.loc[np.repeat(df_test.index.values,len(combinations))]\n",
        "test_data['log_q'] = np.tile(log_q_mesh.ravel(),len(df_test))\n",
        "test_data['theta'] = np.tile(theta_mesh.ravel(),len(df_test))\n",
        "input_ = '/content/drive/MyDrive/CREASE-2D/Crease_2400_126/test/'\n",
        "output_ = '/content/drive/MyDrive/CREASE-2D/Crease_2400_126/downsampled_test_126/'\n",
        "if not os.path.exists(output_):\n",
        "    os.makedirs(output_)\n",
        "for f in os.listdir(input_):\n",
        "    if f.endswith('xy.txt'):\n",
        "        inp_path = os.path.join(input_, f)\n",
        "        print(f\"Processing: {inp_path}\")\n",
        "        out_path = os.path.join(output_, f)\n",
        "        image_data = np.loadtxt(inp_path, delimiter=',')\n",
        "        downsampled = np.zeros((126, 37))\n",
        "        for i in range(126):\n",
        "            for j in range(37):\n",
        "                downsampled[i, j] = image_data[i * 4, j * 5]\n",
        "        np.savetxt(out_path, downsampled, delimiter=',')\n",
        "        print(f\"Saved: {out_path}\")\n",
        "data_folder = \"/content/drive/MyDrive/CREASE-2D/Crease_2400_126/downsampled_test_126/\"\n",
        "data = []\n",
        "for filename in os.listdir(data_folder):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        match = re.search(r'\\d+', filename)\n",
        "        first_number = int(match.group()) if match else None\n",
        "        file_path = os.path.join(data_folder, filename)\n",
        "        matrix_data = []\n",
        "        with open(file_path, \"r\") as txt_file:\n",
        "            for line in txt_file:\n",
        "                values = line.strip().split(\",\")\n",
        "                row = list(map(float, values))\n",
        "                matrix_data.append(row)\n",
        "        flattened_data = [value for row in matrix_data for value in row]\n",
        "        combinations = [(log_q_value, theta_value) for log_q_value in log_q for theta_value in theta]\n",
        "        for idx, (log_q_value, theta_value) in enumerate(combinations):\n",
        "            intensity = flattened_data[idx]\n",
        "            data.append([log_q_value, theta_value, intensity, first_number])\n",
        "df2 = pd.DataFrame(data, columns=[\"log_q\", \"theta\", \"I_q\", \"Sample ID\"])\n",
        "#combine the features, q, theta and intensty to form test dataset\n",
        "test_df = pd.merge(test_data, df2, on=['Sample ID','log_q','theta'], sort=True, copy=False)\n",
        "print(test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('/content/drive/MyDrive/CREASE-2D/Crease_2400_126/test_dataset_2400_126.csv',index=False)\n",
        "train_df.to_csv('/content/drive/MyDrive/CREASE-2D/Crease_2400_126/train_dataset_2400_126.csv',index=False)"
      ],
      "metadata": {
        "id": "V00F2lzKGOWe"
      },
      "id": "V00F2lzKGOWe",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}